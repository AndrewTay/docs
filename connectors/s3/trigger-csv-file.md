---
title: Workato connectors - Amazon S3 trigger - New/updated CSV file
date: 2018-12-13 23:00:00 Z
---

# Amazon S3 trigger - New/updated CSV file
Triggers when a CSV file is added or updated in a selected bucket/folder in Amazon S3.

Checks selected folder for new or updated CSV file every 5 minutes. The output includes the fileâ€™s metadata and file contents, which are CSV rows delivered in batches.

Note that in Amazon S3, when a file is renamed, it is considered a new file. When a file is uploaded and overwrites an existing file with the same name, it is considered an updated file but not a new file.

## Input fields
| Field name | Description |
|---|---|
| When first started, this recipe should pick up events from | When recipe starts for the first time, it will pick up CSV files created or updated from this specified time. Once recipe has been run or tested, value cannot be changed. [Learn more about this field here](https://docs.workato.com/recipes/triggers.html#sincefrom).  |
| Bucket region | The region of the bucket to monitor for new/updated file, e.g. us-west-2. In Amazon S3, go to **Bucket > Properties > Static website hosting** to find your region in the Endpoint URL. |
| Bucket | The bucket to monitor for new/updated CSV file. Select a bucket from the picklist or enter the bucket name directly. |
| Folder | The folder to monitor for new/updated CSV file. Select a folder from the picklist or enter the folder path directly. |
| Include sub-folders | If set to `Yes`, sub-folders will also be monitored for new/updated CSV file. |
| Include files not ending with .csv? | Handle the cases when your CSV files exported from other systems may not have `.csv` extension.|
| Column names | The column names of the CSV file. Upload a sample CSV file to automatically generate column names, or add column names manually. |
| Column delimiter | Delimiter separating the columns in the CSV file. |
| Contains header row? | Set to Yes if CSV file contains header row. Workato will not process that row as data.  |
| Batch size | Workato divides the CSV file into smaller batches to process more efficiently. This field defines the number of CSV rows to process in each batch (Maximum of 1000 rows/batch). Set larger batch size to increase data throughput. In some cases, Workato will automatically reduce batch size to avoid exceeding API limit. [Learn more about Batch Processing](https://docs.workato.com/features/batch-processing.html). |

This trigger supports [Trigger Condition](https://docs.workato.com/recipes/triggers.html#trigger-conditions), which allows you to filter trigger events.

## Output fields
| Field name | Description |
|---|---|
| File name | The name of the file. Note that this is not file path. |
| Last modified | The last modified date/time of the file. |
| E tag | The hash of the file object, generated by Amazon S3. |
| Size | The file size in bytes. |
| Storage class | [Storage class](https://aws.amazon.com/s3/storage-classes/) of this file object. Usually `S3 Standard`. |
| File contents | Contents of the file. This is a [streaming object](https://docs.workato.com/features/file-streaming.html) and can handle unlimited file size. |
| CSV rows | This is a [list datapill](https://docs.workato.com/features/list-management.html) containing list of all rows in the CSV file. |
| - Row number | The number of this CSV row. |
| - CSV columns | Contains all column values in this CSV row. You can use the data pill under this to do mapping. |
| - List size | The number of items in this `CSV rows` list. |
